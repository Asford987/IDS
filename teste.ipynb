{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ExpertModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units, dropout_rate):\n",
    "        super(ExpertModel, self).__init__()\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(input_dim, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = units\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class GateModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, hidden_units, dropout_rate):\n",
    "        super(GateModel, self).__init__()\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(input_dim, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = units\n",
    "        layers.append(nn.Linear(input_dim, num_experts))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.model(x), dim=1)\n",
    "\n",
    "\n",
    "class MixtureOfExperts(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, num_experts, expert_hidden_units, gate_hidden_units, num_active_experts, dropout_rate, learning_rate=1e-3):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.experts = nn.ModuleList([ExpertModel(input_dim, output_dim, expert_hidden_units, dropout_rate) for _ in range(num_experts)])\n",
    "        self.gate = GateModel(input_dim, num_experts, gate_hidden_units, dropout_rate)\n",
    "        self.num_active_experts = num_active_experts\n",
    "        self.expert_usage_count = torch.zeros(num_experts, dtype=torch.float32)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)\n",
    "        gate_output = self.gate(x)\n",
    "\n",
    "        expert_usage_count_adjusted = self.expert_usage_count + 1e-10\n",
    "        importance_scores = gate_output / expert_usage_count_adjusted\n",
    "\n",
    "        top_n_expert_indices = torch.argsort(importance_scores, dim=1, descending=True)[:, :self.num_active_experts]\n",
    "        selected_expert_indices = top_n_expert_indices.view(-1)\n",
    "\n",
    "        self.expert_usage_count += torch.bincount(selected_expert_indices, minlength=len(self.experts)).float()\n",
    "\n",
    "        mask = torch.sum(F.one_hot(top_n_expert_indices, num_classes=len(self.experts)), dim=1)\n",
    "        masked_gate_output = gate_output * mask\n",
    "        normalized_gate_output = masked_gate_output / (torch.sum(masked_gate_output, dim=1, keepdim=True) + 1e-7)\n",
    "\n",
    "        masked_expert_outputs = torch.stack([expert_outputs[:, i] * normalized_gate_output[:, i].unsqueeze(1)\n",
    "                                              for i in range(len(self.experts))], dim=1)\n",
    "        final_output = torch.sum(masked_expert_outputs, dim=1)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        f2_score = fbeta_score(y.cpu().numpy(), preds.cpu().numpy(), beta=2, average='macro')\n",
    "        self.log('val_f2', f2_score, prog_bar=True, sync_dist=True)\n",
    "        return {'val_f2': f2_score}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.1, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_f2',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        self.expert_usage_count = self.expert_usage_count.to(self.device)\n",
    "\n",
    "class ExpertUsageLogger(pl.Callback):\n",
    "    def __init__(self, moe_model):\n",
    "        super(ExpertUsageLogger, self).__init__()\n",
    "        self.moe_model = moe_model\n",
    "        self.expert_usage_history = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        usage_count = self.moe_model.expert_usage_count.clone().cpu().numpy()\n",
    "        self.expert_usage_history.append(usage_count)\n",
    "\n",
    "    def plot_expert_usage(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        usage_history = torch.tensor(self.expert_usage_history)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(usage_history.shape[1]):\n",
    "            plt.plot(usage_history[:, i], label=f'Expert {i}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Expert Usage Count')\n",
    "        plt.title('Expert Usage Over Epochs')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'CIC_IoMT_2024_WiFi_MQTT_train.parquet'\n",
    "test_data_path = 'CIC_IoMT_2024_WiFi_MQTT_test.parquet'\n",
    "usage_ratio=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Header_Length       822146\n",
       "Protocol Type         1441\n",
       "Duration              2684\n",
       "Rate               4174510\n",
       "Srate              4174510\n",
       "Drate                    1\n",
       "fin_flag_number        229\n",
       "syn_flag_number        525\n",
       "rst_flag_number        501\n",
       "psh_flag_number        428\n",
       "ack_flag_number        619\n",
       "ece_flag_number          8\n",
       "cwr_flag_number          6\n",
       "ack_count              563\n",
       "syn_count              986\n",
       "fin_count             2605\n",
       "rst_count            11333\n",
       "HTTP                    33\n",
       "HTTPS                  257\n",
       "DNS                     62\n",
       "Telnet                   6\n",
       "SMTP                     7\n",
       "SSH                     14\n",
       "IRC                      7\n",
       "TCP                    253\n",
       "UDP                    282\n",
       "DHCP                    33\n",
       "ARP                     72\n",
       "ICMP                   227\n",
       "IGMP                    13\n",
       "IPv                     72\n",
       "LLC                     72\n",
       "Tot sum               6502\n",
       "Min                   5117\n",
       "Max                   5287\n",
       "AVG                   5291\n",
       "Std                  13274\n",
       "Tot size              5275\n",
       "IAT                  68531\n",
       "Number                  86\n",
       "Magnitue              2637\n",
       "Radius               12993\n",
       "Covariance          717859\n",
       "Variance               772\n",
       "Weight                  92\n",
       "label                   51\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7160831 entries, 0 to 7160830\n",
      "Data columns (total 46 columns):\n",
      " #   Column           Dtype   \n",
      "---  ------           -----   \n",
      " 0   Header_Length    float32 \n",
      " 1   Protocol Type    float16 \n",
      " 2   Duration         float16 \n",
      " 3   Rate             float32 \n",
      " 4   Srate            float32 \n",
      " 5   Drate            int8    \n",
      " 6   fin_flag_number  float16 \n",
      " 7   syn_flag_number  float16 \n",
      " 8   rst_flag_number  float16 \n",
      " 9   psh_flag_number  float16 \n",
      " 10  ack_flag_number  float16 \n",
      " 11  ece_flag_number  float16 \n",
      " 12  cwr_flag_number  float16 \n",
      " 13  ack_count        float16 \n",
      " 14  syn_count        float16 \n",
      " 15  fin_count        float16 \n",
      " 16  rst_count        float16 \n",
      " 17  HTTP             float16 \n",
      " 18  HTTPS            float16 \n",
      " 19  DNS              float16 \n",
      " 20  Telnet           float16 \n",
      " 21  SMTP             float16 \n",
      " 22  SSH              float16 \n",
      " 23  IRC              float16 \n",
      " 24  TCP              float16 \n",
      " 25  UDP              float16 \n",
      " 26  DHCP             float16 \n",
      " 27  ARP              float16 \n",
      " 28  ICMP             float16 \n",
      " 29  IGMP             float16 \n",
      " 30  IPv              float16 \n",
      " 31  LLC              float16 \n",
      " 32  Tot sum          float16 \n",
      " 33  Min              float16 \n",
      " 34  Max              float16 \n",
      " 35  AVG              float16 \n",
      " 36  Std              float16 \n",
      " 37  Tot size         float16 \n",
      " 38  IAT              float32 \n",
      " 39  Number           float16 \n",
      " 40  Magnitue         float16 \n",
      " 41  Radius           float16 \n",
      " 42  Covariance       float32 \n",
      " 43  Variance         float16 \n",
      " 44  Weight           float16 \n",
      " 45  label            category\n",
      "dtypes: category(1), float16(39), float32(5), int8(1)\n",
      "memory usage: 682.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_parquet(train_data_path)\n",
    "df_test = pd.read_parquet(test_data_path)\n",
    "\n",
    "# Combine train and test data\n",
    "df_combined = pd.concat([df_train, df_test])\n",
    "\n",
    "display(df_train.nunique())\n",
    "df_train.info()\n",
    "# Perform stratified sampling\n",
    "df_sampled, _ = train_test_split(df_combined, train_size=usage_ratio, stratify=df_combined['label'], random_state=42)\n",
    "\n",
    "# Split back into train and test based on the original indices\n",
    "df_train: pd.DataFrame = df_sampled[df_sampled.index.isin(df_train.index)]\n",
    "df_test: pd.DataFrame = df_sampled[df_sampled.index.isin(df_test.index)]\n",
    "numeric_columns = df_train.select_dtypes(include=[np.number]).columns\n",
    "df_train[numeric_columns] = df_train[numeric_columns].astype(np.float32)\n",
    "df_test[numeric_columns] = df_test[numeric_columns].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1755002 entries, 5278661 to 3488986\n",
      "Data columns (total 46 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   Header_Length    float32\n",
      " 1   Protocol Type    float32\n",
      " 2   Duration         float32\n",
      " 3   Rate             float32\n",
      " 4   Srate            float32\n",
      " 5   Drate            float32\n",
      " 6   fin_flag_number  float32\n",
      " 7   syn_flag_number  float32\n",
      " 8   rst_flag_number  float32\n",
      " 9   psh_flag_number  float32\n",
      " 10  ack_flag_number  float32\n",
      " 11  ece_flag_number  float32\n",
      " 12  cwr_flag_number  float32\n",
      " 13  ack_count        float32\n",
      " 14  syn_count        float32\n",
      " 15  fin_count        float32\n",
      " 16  rst_count        float32\n",
      " 17  HTTP             float32\n",
      " 18  HTTPS            float32\n",
      " 19  DNS              float32\n",
      " 20  Telnet           float32\n",
      " 21  SMTP             float32\n",
      " 22  SSH              float32\n",
      " 23  IRC              float32\n",
      " 24  TCP              float32\n",
      " 25  UDP              float32\n",
      " 26  DHCP             float32\n",
      " 27  ARP              float32\n",
      " 28  ICMP             float32\n",
      " 29  IGMP             float32\n",
      " 30  IPv              float32\n",
      " 31  LLC              float32\n",
      " 32  Tot sum          float32\n",
      " 33  Min              float32\n",
      " 34  Max              float32\n",
      " 35  AVG              float32\n",
      " 36  Std              float32\n",
      " 37  Tot size         float32\n",
      " 38  IAT              float32\n",
      " 39  Number           float32\n",
      " 40  Magnitue         float32\n",
      " 41  Radius           float32\n",
      " 42  Covariance       float32\n",
      " 43  Variance         float32\n",
      " 44  Weight           float32\n",
      " 45  label            object \n",
      "dtypes: float32(45), object(1)\n",
      "memory usage: 328.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Header_Length       282568\n",
       "Protocol Type          965\n",
       "Duration              2115\n",
       "Rate               1131368\n",
       "Srate              1131368\n",
       "Drate                    1\n",
       "fin_flag_number        125\n",
       "syn_flag_number        228\n",
       "rst_flag_number        219\n",
       "psh_flag_number        188\n",
       "ack_flag_number        250\n",
       "ece_flag_number          5\n",
       "cwr_flag_number          4\n",
       "ack_count              331\n",
       "syn_count              578\n",
       "fin_count             1835\n",
       "rst_count            10928\n",
       "HTTP                    25\n",
       "HTTPS                  142\n",
       "DNS                     37\n",
       "Telnet                   3\n",
       "SMTP                     4\n",
       "SSH                     10\n",
       "IRC                      4\n",
       "TCP                    143\n",
       "UDP                    152\n",
       "DHCP                    17\n",
       "ARP                     64\n",
       "ICMP                   135\n",
       "IGMP                    12\n",
       "IPv                     64\n",
       "LLC                     64\n",
       "Tot sum               6057\n",
       "Min                   4304\n",
       "Max                   5274\n",
       "AVG                   5240\n",
       "Std                  12737\n",
       "Tot size              5151\n",
       "IAT                  23957\n",
       "Number                  82\n",
       "Magnitue              2621\n",
       "Radius               12480\n",
       "Covariance          195748\n",
       "Variance               289\n",
       "Weight                  88\n",
       "label                   72\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.info()\n",
    "df_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.01000214, 0.60009766, 0.19995117, 0.02000427,\n",
       "       0.39990234, 0.09997559, 0.41992188, 0.2199707 , 0.30004883,\n",
       "       0.36010742, 0.5600586 , 0.23999023, 0.4399414 , 0.13000488,\n",
       "       0.02999878, 0.35009766], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['DHCP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [col for col in df_train.columns if col not in ['label', 'Drate']]\n",
    "\n",
    "target_train = df_train['label']\n",
    "df_train = df_train.drop(columns=['label', 'Drate'])\n",
    "target_test = df_test['label']\n",
    "df_test = df_test.drop(columns=['label', 'Drate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of missing values: {df_train.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>psh_flag_number</th>\n",
       "      <th>ack_flag_number</th>\n",
       "      <th>...</th>\n",
       "      <th>AVG</th>\n",
       "      <th>Std</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1755002.0</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "      <td>1.755002e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.988337e+04</td>\n",
       "      <td>8.046584e+00</td>\n",
       "      <td>6.463663e+01</td>\n",
       "      <td>1.573294e+04</td>\n",
       "      <td>1.573294e+04</td>\n",
       "      <td>5.101282e-03</td>\n",
       "      <td>1.571194e-01</td>\n",
       "      <td>3.962176e-02</td>\n",
       "      <td>2.219948e-02</td>\n",
       "      <td>9.567751e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.053767e+01</td>\n",
       "      <td>6.038682e+00</td>\n",
       "      <td>6.053738e+01</td>\n",
       "      <td>84678376.0</td>\n",
       "      <td>9.498865e+00</td>\n",
       "      <td>1.043581e+01</td>\n",
       "      <td>8.529323e+00</td>\n",
       "      <td>2.367965e+03</td>\n",
       "      <td>9.065043e-02</td>\n",
       "      <td>1.414742e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.814616e+05</td>\n",
       "      <td>6.304724e+00</td>\n",
       "      <td>7.837749e+00</td>\n",
       "      <td>4.000080e+04</td>\n",
       "      <td>4.000080e+04</td>\n",
       "      <td>3.395753e-02</td>\n",
       "      <td>3.367704e-01</td>\n",
       "      <td>1.395269e-01</td>\n",
       "      <td>9.655753e-02</td>\n",
       "      <td>2.522283e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.783871e+01</td>\n",
       "      <td>3.802954e+01</td>\n",
       "      <td>8.758988e+01</td>\n",
       "      <td>17817836.0</td>\n",
       "      <td>8.414046e-01</td>\n",
       "      <td>3.150808e+00</td>\n",
       "      <td>5.376392e+01</td>\n",
       "      <td>1.980274e+04</td>\n",
       "      <td>2.328664e-01</td>\n",
       "      <td>2.166214e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.164062e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.310000e+00</td>\n",
       "      <td>1.049805e+00</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>6.428900e+00</td>\n",
       "      <td>6.428900e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.209375e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.225000e+01</td>\n",
       "      <td>84679176.0</td>\n",
       "      <td>9.500000e+00</td>\n",
       "      <td>9.171875e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.415000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.080000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>1.331385e+02</td>\n",
       "      <td>1.331385e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>84696416.0</td>\n",
       "      <td>9.500000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.415000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.940038e+04</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>1.976398e+04</td>\n",
       "      <td>1.976398e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>84696904.0</td>\n",
       "      <td>9.500000e+00</td>\n",
       "      <td>1.039062e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.415000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.892476e+06</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>2.097152e+06</td>\n",
       "      <td>2.097152e+06</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.514000e+03</td>\n",
       "      <td>7.205000e+02</td>\n",
       "      <td>1.514000e+03</td>\n",
       "      <td>169470848.0</td>\n",
       "      <td>1.350000e+01</td>\n",
       "      <td>5.503125e+01</td>\n",
       "      <td>1.019500e+03</td>\n",
       "      <td>5.197575e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.446250e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Header_Length  Protocol Type      Duration          Rate         Srate  \\\n",
       "count   1.755002e+06   1.755002e+06  1.755002e+06  1.755002e+06  1.755002e+06   \n",
       "mean    2.988337e+04   8.046584e+00  6.463663e+01  1.573294e+04  1.573294e+04   \n",
       "std     2.814616e+05   6.304724e+00  7.837749e+00  4.000080e+04  4.000080e+04   \n",
       "min     0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%     2.310000e+00   1.049805e+00  6.400000e+01  6.428900e+00  6.428900e+00   \n",
       "50%     1.080000e+02   6.000000e+00  6.400000e+01  1.331385e+02  1.331385e+02   \n",
       "75%     1.940038e+04   1.700000e+01  6.400000e+01  1.976398e+04  1.976398e+04   \n",
       "max     9.892476e+06   1.700000e+01  2.550000e+02  2.097152e+06  2.097152e+06   \n",
       "\n",
       "       fin_flag_number  syn_flag_number  rst_flag_number  psh_flag_number  \\\n",
       "count     1.755002e+06     1.755002e+06     1.755002e+06     1.755002e+06   \n",
       "mean      5.101282e-03     1.571194e-01     3.962176e-02     2.219948e-02   \n",
       "std       3.395753e-02     3.367704e-01     1.395269e-01     9.655753e-02   \n",
       "min       0.000000e+00     0.000000e+00     0.000000e+00     0.000000e+00   \n",
       "25%       0.000000e+00     0.000000e+00     0.000000e+00     0.000000e+00   \n",
       "50%       0.000000e+00     0.000000e+00     0.000000e+00     0.000000e+00   \n",
       "75%       0.000000e+00     0.000000e+00     0.000000e+00     0.000000e+00   \n",
       "max       1.000000e+00     1.000000e+00     1.000000e+00     1.000000e+00   \n",
       "\n",
       "       ack_flag_number  ...           AVG           Std      Tot size  \\\n",
       "count     1.755002e+06  ...  1.755002e+06  1.755002e+06  1.755002e+06   \n",
       "mean      9.567751e-02  ...  6.053767e+01  6.038682e+00  6.053738e+01   \n",
       "std       2.522283e-01  ...  8.783871e+01  3.802954e+01  8.758988e+01   \n",
       "min       0.000000e+00  ...  4.200000e+01  0.000000e+00  4.200000e+01   \n",
       "25%       0.000000e+00  ...  4.209375e+01  0.000000e+00  4.225000e+01   \n",
       "50%       0.000000e+00  ...  5.000000e+01  0.000000e+00  5.000000e+01   \n",
       "75%       0.000000e+00  ...  5.400000e+01  0.000000e+00  5.400000e+01   \n",
       "max       1.000000e+00  ...  1.514000e+03  7.205000e+02  1.514000e+03   \n",
       "\n",
       "               IAT        Number      Magnitue        Radius    Covariance  \\\n",
       "count    1755002.0  1.755002e+06  1.755002e+06  1.755002e+06  1.755002e+06   \n",
       "mean    84678376.0  9.498865e+00  1.043581e+01  8.529323e+00  2.367965e+03   \n",
       "std     17817836.0  8.414046e-01  3.150808e+00  5.376392e+01  1.980274e+04   \n",
       "min            0.0  1.000000e+00  9.164062e+00  0.000000e+00  0.000000e+00   \n",
       "25%     84679176.0  9.500000e+00  9.171875e+00  0.000000e+00  0.000000e+00   \n",
       "50%     84696416.0  9.500000e+00  1.000000e+01  0.000000e+00  0.000000e+00   \n",
       "75%     84696904.0  9.500000e+00  1.039062e+01  0.000000e+00  0.000000e+00   \n",
       "max    169470848.0  1.350000e+01  5.503125e+01  1.019500e+03  5.197575e+05   \n",
       "\n",
       "           Variance        Weight  \n",
       "count  1.755002e+06  1.755002e+06  \n",
       "mean   9.065043e-02  1.414742e+02  \n",
       "std    2.328664e-01  2.166214e+01  \n",
       "min    0.000000e+00  1.000000e+00  \n",
       "25%    0.000000e+00  1.415000e+02  \n",
       "50%    0.000000e+00  1.415000e+02  \n",
       "75%    0.000000e+00  1.415000e+02  \n",
       "max    1.000000e+00  2.446250e+02  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "target_train_encoded = encoder.fit_transform(target_train.values.reshape(-1, 1))\n",
    "target_test_encoded = encoder.transform(target_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1755002, 1), (645668, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train_encoded.shape, target_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185096 out of 1755002 samples were filtered out as outliers.\n",
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "z_scores = np.abs(stats.zscore(df_train[numerical_columns].astype(np.float64)))\n",
    "\n",
    "outlier_mask = np.any(z_scores > 4, axis=1)\n",
    "\n",
    "# Filter out rows with outliers\n",
    "df_train = df_train[~outlier_mask]\n",
    "target_train_encoded = target_train_encoded[~outlier_mask]\n",
    "\n",
    "print(f\"{outlier_mask.sum()} out of {len(outlier_mask)} samples were filtered out as outliers.\")\n",
    "print(f\"Number of missing values: {df_train.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Srate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>psh_flag_number</th>\n",
       "      <th>ack_flag_number</th>\n",
       "      <th>...</th>\n",
       "      <th>AVG</th>\n",
       "      <th>Std</th>\n",
       "      <th>Tot size</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Magnitue</th>\n",
       "      <th>Radius</th>\n",
       "      <th>Covariance</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>452212</th>\n",
       "      <td>21313.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>18876.460938</td>\n",
       "      <td>18876.460938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>84696904.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3101967</th>\n",
       "      <td>69.660004</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>78.480476</td>\n",
       "      <td>78.480476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.569824</td>\n",
       "      <td>0.429932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>84696408.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.390625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640695</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>30727.501953</td>\n",
       "      <td>30727.501953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>84697064.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.164062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696721</th>\n",
       "      <td>116.320000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.448729</td>\n",
       "      <td>7.448729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469971</td>\n",
       "      <td>0.529785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449951</td>\n",
       "      <td>...</td>\n",
       "      <td>55.62500</td>\n",
       "      <td>0.071106</td>\n",
       "      <td>55.81250</td>\n",
       "      <td>84674736.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.546875</td>\n",
       "      <td>0.099121</td>\n",
       "      <td>0.101392</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628390</th>\n",
       "      <td>108.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.584863</td>\n",
       "      <td>0.584863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>84670888.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.390625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795740</th>\n",
       "      <td>121.739998</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>52.114407</td>\n",
       "      <td>52.114407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.990234</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>...</td>\n",
       "      <td>56.40625</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>56.40625</td>\n",
       "      <td>84696416.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.617188</td>\n",
       "      <td>0.674805</td>\n",
       "      <td>0.641260</td>\n",
       "      <td>0.379883</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746787</th>\n",
       "      <td>108.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.546219</td>\n",
       "      <td>0.546219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>84670992.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.390625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261689</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>163.638641</td>\n",
       "      <td>163.638641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>84696136.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.390625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965585</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.001906</td>\n",
       "      <td>3.001906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>84696976.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.164062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732033</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>110376.421875</td>\n",
       "      <td>110376.421875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>84696936.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.164062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5501170</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>54.937111</td>\n",
       "      <td>54.937111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>84696008.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.390625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618233</th>\n",
       "      <td>113.620003</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.971550</td>\n",
       "      <td>5.971550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.810059</td>\n",
       "      <td>0.189941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160034</td>\n",
       "      <td>...</td>\n",
       "      <td>54.71875</td>\n",
       "      <td>0.379639</td>\n",
       "      <td>54.62500</td>\n",
       "      <td>84674736.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.460938</td>\n",
       "      <td>0.537598</td>\n",
       "      <td>0.701166</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634604</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6.393269</td>\n",
       "      <td>6.393269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>84674664.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.390625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363491</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.948594</td>\n",
       "      <td>3.948594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.00000</td>\n",
       "      <td>84696040.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.390625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882136</th>\n",
       "      <td>40975.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20276.767578</td>\n",
       "      <td>20276.767578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>84675328.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>141.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Header_Length  Protocol Type  Duration           Rate          Srate  \\\n",
       "452212    21313.000000           17.0      64.0   18876.460938   18876.460938   \n",
       "3101967      69.660004            6.0      64.0      78.480476      78.480476   \n",
       "1640695       0.000000            1.0      64.0   30727.501953   30727.501953   \n",
       "696721      116.320000            6.0      64.0       7.448729       7.448729   \n",
       "2628390     108.000000            6.0      64.0       0.584863       0.584863   \n",
       "1795740     121.739998            6.0      64.0      52.114407      52.114407   \n",
       "746787      108.000000            6.0      64.0       0.546219       0.546219   \n",
       "261689       54.000000            6.0      64.0     163.638641     163.638641   \n",
       "965585        0.000000            1.0      64.0       3.001906       3.001906   \n",
       "4732033       0.000000            1.0      64.0  110376.421875  110376.421875   \n",
       "5501170      54.000000            6.0      64.0      54.937111      54.937111   \n",
       "618233      113.620003            6.0      64.0       5.971550       5.971550   \n",
       "634604       54.000000            6.0      64.0       6.393269       6.393269   \n",
       "2363491      54.000000            6.0      64.0       3.948594       3.948594   \n",
       "5882136   40975.000000           17.0      64.0   20276.767578   20276.767578   \n",
       "\n",
       "         fin_flag_number  syn_flag_number  rst_flag_number  psh_flag_number  \\\n",
       "452212               0.0         0.000000         0.000000              0.0   \n",
       "3101967              0.0         0.569824         0.429932              0.0   \n",
       "1640695              0.0         0.000000         0.000000              0.0   \n",
       "696721               0.0         0.469971         0.529785              0.0   \n",
       "2628390              0.0         0.000000         0.000000              0.0   \n",
       "1795740              0.0         0.990234         0.010002              0.0   \n",
       "746787               0.0         0.000000         0.000000              0.0   \n",
       "261689               0.0         0.000000         0.000000              0.0   \n",
       "965585               0.0         0.000000         0.000000              0.0   \n",
       "4732033              0.0         0.000000         0.000000              0.0   \n",
       "5501170              0.0         0.000000         0.000000              0.0   \n",
       "618233               0.0         0.810059         0.189941              0.0   \n",
       "634604               0.0         1.000000         0.000000              0.0   \n",
       "2363491              0.0         0.000000         0.000000              0.0   \n",
       "5882136              0.0         0.000000         0.000000              0.0   \n",
       "\n",
       "         ack_flag_number  ...       AVG       Std  Tot size         IAT  \\\n",
       "452212          0.000000  ...  50.00000  0.000000  50.00000  84696904.0   \n",
       "3101967         0.000000  ...  54.00000  0.000000  54.00000  84696408.0   \n",
       "1640695         0.000000  ...  42.00000  0.000000  42.00000  84697064.0   \n",
       "696721          0.449951  ...  55.62500  0.071106  55.81250  84674736.0   \n",
       "2628390         0.000000  ...  54.00000  0.000000  54.00000  84670888.0   \n",
       "1795740         0.600098  ...  56.40625  0.476562  56.40625  84696416.0   \n",
       "746787          0.000000  ...  54.00000  0.000000  54.00000  84670992.0   \n",
       "261689          0.000000  ...  54.00000  0.000000  54.00000  84696136.0   \n",
       "965585          0.000000  ...  42.00000  0.000000  42.00000  84696976.0   \n",
       "4732033         0.000000  ...  42.00000  0.000000  42.00000  84696936.0   \n",
       "5501170         0.000000  ...  54.00000  0.000000  54.00000  84696008.0   \n",
       "618233          0.160034  ...  54.71875  0.379639  54.62500  84674736.0   \n",
       "634604          0.000000  ...  54.00000  0.000000  54.00000  84674664.0   \n",
       "2363491         0.000000  ...  54.00000  0.000000  54.00000  84696040.0   \n",
       "5882136         0.000000  ...  50.00000  0.000000  50.00000  84675328.0   \n",
       "\n",
       "         Number   Magnitue    Radius  Covariance  Variance  Weight  \n",
       "452212      9.5  10.000000  0.000000    0.000000  0.000000   141.5  \n",
       "3101967     9.5  10.390625  0.000000    0.000000  0.000000   141.5  \n",
       "1640695     9.5   9.164062  0.000000    0.000000  0.000000   141.5  \n",
       "696721      9.5  10.546875  0.099121    0.101392  0.049988   141.5  \n",
       "2628390     9.5  10.390625  0.000000    0.000000  0.000000   141.5  \n",
       "1795740     9.5  10.617188  0.674805    0.641260  0.379883   141.5  \n",
       "746787      9.5  10.390625  0.000000    0.000000  0.000000   141.5  \n",
       "261689      9.5  10.390625  0.000000    0.000000  0.000000   141.5  \n",
       "965585      9.5   9.164062  0.000000    0.000000  0.000000   141.5  \n",
       "4732033     9.5   9.164062  0.000000    0.000000  0.000000   141.5  \n",
       "5501170     9.5  10.390625  0.000000    0.000000  0.000000   141.5  \n",
       "618233      9.5  10.460938  0.537598    0.701166  0.209961   141.5  \n",
       "634604      9.5  10.390625  0.000000    0.000000  0.000000   141.5  \n",
       "2363491     9.5  10.390625  0.000000    0.000000  0.000000   141.5  \n",
       "5882136     9.5  10.000000  0.000000    0.000000  0.000000   141.5  \n",
       "\n",
       "[15 rows x 44 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Header_Length      8.671483e+03\n",
       "Protocol Type      8.133665e+00\n",
       "Duration           6.429772e+01\n",
       "Rate               1.382555e+04\n",
       "Srate              1.382555e+04\n",
       "fin_flag_number    2.179058e-04\n",
       "syn_flag_number    1.580998e-01\n",
       "rst_flag_number    2.131322e-02\n",
       "psh_flag_number    1.313877e-03\n",
       "ack_flag_number    3.916546e-02\n",
       "ece_flag_number    0.000000e+00\n",
       "cwr_flag_number    0.000000e+00\n",
       "ack_count          1.003523e-03\n",
       "syn_count          2.447479e-01\n",
       "fin_count          2.861828e-02\n",
       "rst_count          6.779028e-01\n",
       "HTTP               8.658214e-06\n",
       "HTTPS              8.020909e-04\n",
       "DNS                2.159189e-05\n",
       "Telnet             0.000000e+00\n",
       "SMTP               0.000000e+00\n",
       "SSH                2.421044e-07\n",
       "IRC                0.000000e+00\n",
       "TCP                3.660790e-01\n",
       "UDP                3.314583e-01\n",
       "DHCP               0.000000e+00\n",
       "ARP                4.128990e-05\n",
       "ICMP               3.024321e-01\n",
       "IGMP               0.000000e+00\n",
       "IPv                9.999593e-01\n",
       "LLC                9.999593e-01\n",
       "Tot sum            5.256331e+02\n",
       "Min                4.940793e+01\n",
       "Max                5.243468e+01\n",
       "AVG                5.005311e+01\n",
       "Std                1.005190e+00\n",
       "Tot size           5.007597e+01\n",
       "IAT                8.469190e+07\n",
       "Number             9.499704e+00\n",
       "Magnitue           9.951477e+00\n",
       "Radius             1.417852e+00\n",
       "Covariance         3.207044e+02\n",
       "Variance           3.419412e-02\n",
       "Weight             1.414920e+02\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Header_Length       14451.265625\n",
       "Protocol Type           6.540149\n",
       "Duration                2.056136\n",
       "Rate                25310.832031\n",
       "Srate               25310.832031\n",
       "fin_flag_number         0.004668\n",
       "syn_flag_number         0.344913\n",
       "rst_flag_number         0.084044\n",
       "psh_flag_number         0.016479\n",
       "ack_flag_number         0.145640\n",
       "ece_flag_number         0.000000\n",
       "cwr_flag_number         0.000000\n",
       "ack_count               0.022014\n",
       "syn_count               0.546714\n",
       "fin_count               0.109391\n",
       "rst_count              11.508127\n",
       "HTTP                    0.000392\n",
       "HTTPS                   0.008465\n",
       "DNS                     0.000537\n",
       "Telnet                  0.000000\n",
       "SMTP                    0.000000\n",
       "SSH                     0.000049\n",
       "IRC                     0.000000\n",
       "TCP                     0.480650\n",
       "UDP                     0.468974\n",
       "DHCP                    0.000000\n",
       "ARP                     0.000879\n",
       "ICMP                    0.457853\n",
       "IGMP                    0.000000\n",
       "IPv                     0.000871\n",
       "LLC                     0.000871\n",
       "Tot sum               118.633339\n",
       "Min                     7.436850\n",
       "Max                    23.801250\n",
       "AVG                    11.423596\n",
       "Std                     7.452578\n",
       "Tot size               11.207402\n",
       "IAT                601276.437500\n",
       "Number                  0.015403\n",
       "Magnitue                0.678651\n",
       "Radius                 10.521799\n",
       "Covariance           3770.656494\n",
       "Variance                0.100514\n",
       "Weight                  0.390315\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean = df_train.mean()\n",
    "std = df_train.std()\n",
    "display(mean)\n",
    "display(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n",
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "mean = mean + 1e-5\n",
    "std = std + 1e-5\n",
    "df_train = ((df_train - mean) / std).dropna(axis=1)\n",
    "df_test = ((df_test - mean) / std).dropna(axis=1)\n",
    "\n",
    "print(f\"Number of missing values: {df_train.isna().sum().sum()}\")\n",
    "print(f\"Number of missing values: {df_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(df_train, rowvar=False)\n",
    "upper_triangle_indices = np.triu_indices_from(corr_matrix, k=1)\n",
    "correlated_pairs = [(i, j) for i, j in zip(*upper_triangle_indices) if np.abs(corr_matrix[i, j]) >= 0.8]\n",
    "cols_train, cols_test = df_train.columns, df_test.columns\n",
    "correlated_features = set(j for _, j in correlated_pairs)\n",
    "df_train = np.delete(df_train, list(correlated_features), axis=1)\n",
    "df_test = np.delete(df_test, list(correlated_features), axis=1)\n",
    "df_train = pd.DataFrame(df_train, columns=cols_train.drop(cols_train[list(correlated_features)]))\n",
    "df_test = pd.DataFrame(df_test, columns=cols_test.drop(cols_test[list(correlated_features)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate',\n",
       "       'fin_flag_number', 'syn_flag_number', 'rst_flag_number',\n",
       "       'psh_flag_number', 'ack_flag_number', 'ece_flag_number',\n",
       "       'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count',\n",
       "       'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP',\n",
       "       'DHCP', 'ARP', 'ICMP', 'IGMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max',\n",
       "       'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius',\n",
       "       'Covariance', 'Variance', 'Weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header_Length</th>\n",
       "      <th>Protocol Type</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Rate</th>\n",
       "      <th>fin_flag_number</th>\n",
       "      <th>syn_flag_number</th>\n",
       "      <th>rst_flag_number</th>\n",
       "      <th>psh_flag_number</th>\n",
       "      <th>ack_flag_number</th>\n",
       "      <th>ece_flag_number</th>\n",
       "      <th>...</th>\n",
       "      <th>IRC</th>\n",
       "      <th>TCP</th>\n",
       "      <th>DHCP</th>\n",
       "      <th>ARP</th>\n",
       "      <th>ICMP</th>\n",
       "      <th>IGMP</th>\n",
       "      <th>Tot sum</th>\n",
       "      <th>IAT</th>\n",
       "      <th>Number</th>\n",
       "      <th>Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504455</td>\n",
       "      <td>1.355674</td>\n",
       "      <td>-0.144800</td>\n",
       "      <td>-0.032098</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.761638</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>-0.660552</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.005337</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.592524</td>\n",
       "      <td>-0.326242</td>\n",
       "      <td>-0.144800</td>\n",
       "      <td>-0.545966</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>1.541884</td>\n",
       "      <td>3.435111</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>2.272168</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.318834</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>-0.660552</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.487779</td>\n",
       "      <td>-0.028553</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.041834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.850342</td>\n",
       "      <td>1.355674</td>\n",
       "      <td>-0.144800</td>\n",
       "      <td>0.249141</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.761638</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>-0.660552</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.005337</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.986055</td>\n",
       "      <td>1.355674</td>\n",
       "      <td>-0.144800</td>\n",
       "      <td>-0.095234</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.761638</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>-0.660552</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.005337</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.947427</td>\n",
       "      <td>1.135878</td>\n",
       "      <td>8.214285</td>\n",
       "      <td>0.517582</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.761638</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>-0.463928</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.218041</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>2.246288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569901</th>\n",
       "      <td>-0.592577</td>\n",
       "      <td>-0.326242</td>\n",
       "      <td>-0.144800</td>\n",
       "      <td>-0.546201</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.318834</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>-0.660552</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.348695</td>\n",
       "      <td>-0.034793</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569902</th>\n",
       "      <td>-0.600050</td>\n",
       "      <td>-1.090750</td>\n",
       "      <td>-0.144800</td>\n",
       "      <td>-0.546073</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.761638</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>1.523509</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.713401</td>\n",
       "      <td>0.015115</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569903</th>\n",
       "      <td>2.093589</td>\n",
       "      <td>1.355674</td>\n",
       "      <td>-0.144800</td>\n",
       "      <td>0.136104</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.761638</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>-0.660552</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.005337</td>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569904</th>\n",
       "      <td>-0.600050</td>\n",
       "      <td>-1.090750</td>\n",
       "      <td>0.797497</td>\n",
       "      <td>-0.546073</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.761638</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>1.523509</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.713401</td>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569905</th>\n",
       "      <td>-0.600050</td>\n",
       "      <td>-1.090750</td>\n",
       "      <td>-0.144800</td>\n",
       "      <td>0.685828</td>\n",
       "      <td>-0.048715</td>\n",
       "      <td>-0.458391</td>\n",
       "      <td>-0.253684</td>\n",
       "      <td>-0.080287</td>\n",
       "      <td>-0.268969</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.761638</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0577</td>\n",
       "      <td>1.523509</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.713401</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>-0.340259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569906 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Header_Length  Protocol Type  Duration      Rate  fin_flag_number  \\\n",
       "0             0.504455       1.355674 -0.144800 -0.032098        -0.048715   \n",
       "1            -0.592524      -0.326242 -0.144800 -0.545966        -0.048715   \n",
       "2             0.850342       1.355674 -0.144800  0.249141        -0.048715   \n",
       "3             1.986055       1.355674 -0.144800 -0.095234        -0.048715   \n",
       "4             0.947427       1.135878  8.214285  0.517582        -0.048715   \n",
       "...                ...            ...       ...       ...              ...   \n",
       "1569901      -0.592577      -0.326242 -0.144800 -0.546201        -0.048715   \n",
       "1569902      -0.600050      -1.090750 -0.144800 -0.546073        -0.048715   \n",
       "1569903       2.093589       1.355674 -0.144800  0.136104        -0.048715   \n",
       "1569904      -0.600050      -1.090750  0.797497 -0.546073        -0.048715   \n",
       "1569905      -0.600050      -1.090750 -0.144800  0.685828        -0.048715   \n",
       "\n",
       "         syn_flag_number  rst_flag_number  psh_flag_number  ack_flag_number  \\\n",
       "0              -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "1               1.541884         3.435111        -0.080287         2.272168   \n",
       "2              -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "3              -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "4              -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "...                  ...              ...              ...              ...   \n",
       "1569901        -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "1569902        -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "1569903        -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "1569904        -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "1569905        -0.458391        -0.253684        -0.080287        -0.268969   \n",
       "\n",
       "         ece_flag_number  ...  IRC       TCP  DHCP     ARP      ICMP  IGMP  \\\n",
       "0                   -1.0  ... -1.0 -0.761638  -1.0 -0.0577 -0.660552  -1.0   \n",
       "1                   -1.0  ... -1.0  1.318834  -1.0 -0.0577 -0.660552  -1.0   \n",
       "2                   -1.0  ... -1.0 -0.761638  -1.0 -0.0577 -0.660552  -1.0   \n",
       "3                   -1.0  ... -1.0 -0.761638  -1.0 -0.0577 -0.660552  -1.0   \n",
       "4                   -1.0  ... -1.0 -0.761638  -1.0 -0.0577 -0.463928  -1.0   \n",
       "...                  ...  ...  ...       ...   ...     ...       ...   ...   \n",
       "1569901             -1.0  ... -1.0  1.318834  -1.0 -0.0577 -0.660552  -1.0   \n",
       "1569902             -1.0  ... -1.0 -0.761638  -1.0 -0.0577  1.523509  -1.0   \n",
       "1569903             -1.0  ... -1.0 -0.761638  -1.0 -0.0577 -0.660552  -1.0   \n",
       "1569904             -1.0  ... -1.0 -0.761638  -1.0 -0.0577  1.523509  -1.0   \n",
       "1569905             -1.0  ... -1.0 -0.761638  -1.0 -0.0577  1.523509  -1.0   \n",
       "\n",
       "          Tot sum       IAT    Number  Variance  \n",
       "0       -0.005337  0.008236  0.018562 -0.340259  \n",
       "1        0.487779 -0.028553  0.018562 -0.041834  \n",
       "2       -0.005337  0.008249  0.018562 -0.340259  \n",
       "3       -0.005337  0.008276  0.018562 -0.340259  \n",
       "4        0.218041  0.007956  0.018562  2.246288  \n",
       "...           ...       ...       ...       ...  \n",
       "1569901  0.348695 -0.034793  0.018562 -0.340259  \n",
       "1569902 -0.713401  0.015115  0.018562 -0.340259  \n",
       "1569903 -0.005337  0.007903  0.018562 -0.340259  \n",
       "1569904 -0.713401  0.008435  0.018562 -0.340259  \n",
       "1569905 -0.713401  0.008475  0.018562 -0.340259  \n",
       "\n",
       "[1569906 rows x 28 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "      <th>PC12</th>\n",
       "      <th>PC13</th>\n",
       "      <th>PC14</th>\n",
       "      <th>PC15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.723982</td>\n",
       "      <td>1.405642</td>\n",
       "      <td>-0.509183</td>\n",
       "      <td>-0.043489</td>\n",
       "      <td>-0.453891</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>-0.045733</td>\n",
       "      <td>0.017456</td>\n",
       "      <td>0.019915</td>\n",
       "      <td>0.054392</td>\n",
       "      <td>-0.470635</td>\n",
       "      <td>-0.216196</td>\n",
       "      <td>-0.012250</td>\n",
       "      <td>-0.084373</td>\n",
       "      <td>-0.001162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.298144</td>\n",
       "      <td>-1.425085</td>\n",
       "      <td>-1.520087</td>\n",
       "      <td>-0.024649</td>\n",
       "      <td>-0.111459</td>\n",
       "      <td>0.825453</td>\n",
       "      <td>-0.316069</td>\n",
       "      <td>-0.604433</td>\n",
       "      <td>-0.368913</td>\n",
       "      <td>-1.495695</td>\n",
       "      <td>-0.013148</td>\n",
       "      <td>-0.637633</td>\n",
       "      <td>-0.093273</td>\n",
       "      <td>0.901276</td>\n",
       "      <td>0.023981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.785003</td>\n",
       "      <td>1.657681</td>\n",
       "      <td>-0.506929</td>\n",
       "      <td>-0.050599</td>\n",
       "      <td>-0.518908</td>\n",
       "      <td>0.079913</td>\n",
       "      <td>-0.053267</td>\n",
       "      <td>-0.029689</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>-0.039560</td>\n",
       "      <td>-0.289287</td>\n",
       "      <td>-0.179887</td>\n",
       "      <td>-0.011793</td>\n",
       "      <td>-0.104393</td>\n",
       "      <td>-0.001320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.802038</td>\n",
       "      <td>2.211745</td>\n",
       "      <td>-0.546086</td>\n",
       "      <td>-0.053113</td>\n",
       "      <td>-0.505308</td>\n",
       "      <td>0.063077</td>\n",
       "      <td>-0.056795</td>\n",
       "      <td>-0.051315</td>\n",
       "      <td>-0.048846</td>\n",
       "      <td>-0.215653</td>\n",
       "      <td>-0.828292</td>\n",
       "      <td>-0.296404</td>\n",
       "      <td>-0.022592</td>\n",
       "      <td>-0.271130</td>\n",
       "      <td>-0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.544230</td>\n",
       "      <td>2.852851</td>\n",
       "      <td>1.881443</td>\n",
       "      <td>0.427168</td>\n",
       "      <td>2.744118</td>\n",
       "      <td>5.206410</td>\n",
       "      <td>-0.559283</td>\n",
       "      <td>0.089575</td>\n",
       "      <td>0.541004</td>\n",
       "      <td>1.320186</td>\n",
       "      <td>-1.332039</td>\n",
       "      <td>4.079731</td>\n",
       "      <td>0.281829</td>\n",
       "      <td>1.283006</td>\n",
       "      <td>-0.026053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569901</th>\n",
       "      <td>0.343384</td>\n",
       "      <td>-0.512552</td>\n",
       "      <td>-0.687967</td>\n",
       "      <td>0.027071</td>\n",
       "      <td>0.303698</td>\n",
       "      <td>-0.851152</td>\n",
       "      <td>0.029134</td>\n",
       "      <td>0.315244</td>\n",
       "      <td>0.179364</td>\n",
       "      <td>0.798662</td>\n",
       "      <td>-0.150879</td>\n",
       "      <td>0.100868</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>0.678842</td>\n",
       "      <td>0.036834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569902</th>\n",
       "      <td>-1.247781</td>\n",
       "      <td>-1.478646</td>\n",
       "      <td>1.264406</td>\n",
       "      <td>0.045499</td>\n",
       "      <td>0.215390</td>\n",
       "      <td>-0.159233</td>\n",
       "      <td>-0.059426</td>\n",
       "      <td>-0.042416</td>\n",
       "      <td>-0.043150</td>\n",
       "      <td>-0.242519</td>\n",
       "      <td>-0.386797</td>\n",
       "      <td>-0.291047</td>\n",
       "      <td>-0.020665</td>\n",
       "      <td>-0.172150</td>\n",
       "      <td>0.014284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569903</th>\n",
       "      <td>-0.841816</td>\n",
       "      <td>2.321149</td>\n",
       "      <td>-0.540101</td>\n",
       "      <td>-0.057954</td>\n",
       "      <td>-0.551259</td>\n",
       "      <td>0.107866</td>\n",
       "      <td>-0.061544</td>\n",
       "      <td>-0.081089</td>\n",
       "      <td>-0.057524</td>\n",
       "      <td>-0.259889</td>\n",
       "      <td>-0.643144</td>\n",
       "      <td>-0.258358</td>\n",
       "      <td>-0.021054</td>\n",
       "      <td>-0.265889</td>\n",
       "      <td>-0.000574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569904</th>\n",
       "      <td>-1.212118</td>\n",
       "      <td>-1.348980</td>\n",
       "      <td>1.454567</td>\n",
       "      <td>0.096419</td>\n",
       "      <td>0.600753</td>\n",
       "      <td>0.317209</td>\n",
       "      <td>-0.123615</td>\n",
       "      <td>-0.013282</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>-0.074642</td>\n",
       "      <td>-0.526200</td>\n",
       "      <td>0.211604</td>\n",
       "      <td>0.017073</td>\n",
       "      <td>0.029616</td>\n",
       "      <td>0.014714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569905</th>\n",
       "      <td>-1.425906</td>\n",
       "      <td>-1.212851</td>\n",
       "      <td>1.309646</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>-0.004601</td>\n",
       "      <td>0.057258</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.171818</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.371161</td>\n",
       "      <td>0.715560</td>\n",
       "      <td>-0.061980</td>\n",
       "      <td>-0.008712</td>\n",
       "      <td>-0.073982</td>\n",
       "      <td>0.017838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569906 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0       -0.723982  1.405642 -0.509183 -0.043489 -0.453891  0.017190 -0.045733   \n",
       "1        3.298144 -1.425085 -1.520087 -0.024649 -0.111459  0.825453 -0.316069   \n",
       "2       -0.785003  1.657681 -0.506929 -0.050599 -0.518908  0.079913 -0.053267   \n",
       "3       -0.802038  2.211745 -0.546086 -0.053113 -0.505308  0.063077 -0.056795   \n",
       "4        0.544230  2.852851  1.881443  0.427168  2.744118  5.206410 -0.559283   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1569901  0.343384 -0.512552 -0.687967  0.027071  0.303698 -0.851152  0.029134   \n",
       "1569902 -1.247781 -1.478646  1.264406  0.045499  0.215390 -0.159233 -0.059426   \n",
       "1569903 -0.841816  2.321149 -0.540101 -0.057954 -0.551259  0.107866 -0.061544   \n",
       "1569904 -1.212118 -1.348980  1.454567  0.096419  0.600753  0.317209 -0.123615   \n",
       "1569905 -1.425906 -1.212851  1.309646  0.020592 -0.004601  0.057258 -0.080021   \n",
       "\n",
       "              PC8       PC9      PC10      PC11      PC12      PC13      PC14  \\\n",
       "0        0.017456  0.019915  0.054392 -0.470635 -0.216196 -0.012250 -0.084373   \n",
       "1       -0.604433 -0.368913 -1.495695 -0.013148 -0.637633 -0.093273  0.901276   \n",
       "2       -0.029689 -0.000764 -0.039560 -0.289287 -0.179887 -0.011793 -0.104393   \n",
       "3       -0.051315 -0.048846 -0.215653 -0.828292 -0.296404 -0.022592 -0.271130   \n",
       "4        0.089575  0.541004  1.320186 -1.332039  4.079731  0.281829  1.283006   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1569901  0.315244  0.179364  0.798662 -0.150879  0.100868  0.025759  0.678842   \n",
       "1569902 -0.042416 -0.043150 -0.242519 -0.386797 -0.291047 -0.020665 -0.172150   \n",
       "1569903 -0.081089 -0.057524 -0.259889 -0.643144 -0.258358 -0.021054 -0.265889   \n",
       "1569904 -0.013282  0.024876 -0.074642 -0.526200  0.211604  0.017073  0.029616   \n",
       "1569905 -0.171818 -0.062403 -0.371161  0.715560 -0.061980 -0.008712 -0.073982   \n",
       "\n",
       "             PC15  \n",
       "0       -0.001162  \n",
       "1        0.023981  \n",
       "2       -0.001320  \n",
       "3       -0.000658  \n",
       "4       -0.026053  \n",
       "...           ...  \n",
       "1569901  0.036834  \n",
       "1569902  0.014284  \n",
       "1569903 -0.000574  \n",
       "1569904  0.014714  \n",
       "1569905  0.017838  \n",
       "\n",
       "[1569906 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "      <th>PC12</th>\n",
       "      <th>PC13</th>\n",
       "      <th>PC14</th>\n",
       "      <th>PC15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.785003</td>\n",
       "      <td>1.657681</td>\n",
       "      <td>-0.506929</td>\n",
       "      <td>-0.050599</td>\n",
       "      <td>-0.518908</td>\n",
       "      <td>0.079913</td>\n",
       "      <td>-0.053267</td>\n",
       "      <td>-0.029689</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>-0.039560</td>\n",
       "      <td>-0.289287</td>\n",
       "      <td>-0.179887</td>\n",
       "      <td>-0.011793</td>\n",
       "      <td>-0.104393</td>\n",
       "      <td>-0.001320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.802038</td>\n",
       "      <td>2.211745</td>\n",
       "      <td>-0.546086</td>\n",
       "      <td>-0.053113</td>\n",
       "      <td>-0.505308</td>\n",
       "      <td>0.063077</td>\n",
       "      <td>-0.056795</td>\n",
       "      <td>-0.051315</td>\n",
       "      <td>-0.048846</td>\n",
       "      <td>-0.215653</td>\n",
       "      <td>-0.828292</td>\n",
       "      <td>-0.296404</td>\n",
       "      <td>-0.022592</td>\n",
       "      <td>-0.271130</td>\n",
       "      <td>-0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.726011</td>\n",
       "      <td>1.198838</td>\n",
       "      <td>-0.491408</td>\n",
       "      <td>-0.043707</td>\n",
       "      <td>-0.470147</td>\n",
       "      <td>0.034556</td>\n",
       "      <td>-0.045349</td>\n",
       "      <td>0.019654</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.117939</td>\n",
       "      <td>-0.202292</td>\n",
       "      <td>-0.158644</td>\n",
       "      <td>-0.007374</td>\n",
       "      <td>-0.013347</td>\n",
       "      <td>-0.001263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.772734</td>\n",
       "      <td>1.457680</td>\n",
       "      <td>-0.495099</td>\n",
       "      <td>-0.049000</td>\n",
       "      <td>-0.515639</td>\n",
       "      <td>0.077905</td>\n",
       "      <td>-0.051312</td>\n",
       "      <td>-0.017597</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>-0.143721</td>\n",
       "      <td>-0.148087</td>\n",
       "      <td>-0.008512</td>\n",
       "      <td>-0.050619</td>\n",
       "      <td>-0.001514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.547853</td>\n",
       "      <td>12.327210</td>\n",
       "      <td>23.732811</td>\n",
       "      <td>279.533905</td>\n",
       "      <td>-44.643639</td>\n",
       "      <td>-22.102262</td>\n",
       "      <td>-0.986734</td>\n",
       "      <td>0.446772</td>\n",
       "      <td>-3.180127</td>\n",
       "      <td>-13.028477</td>\n",
       "      <td>-4.967493</td>\n",
       "      <td>14.304883</td>\n",
       "      <td>0.846526</td>\n",
       "      <td>7.171854</td>\n",
       "      <td>83.742699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645663</th>\n",
       "      <td>-0.760433</td>\n",
       "      <td>1.548975</td>\n",
       "      <td>-0.507242</td>\n",
       "      <td>-0.047967</td>\n",
       "      <td>-0.493170</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>-0.050211</td>\n",
       "      <td>-0.010580</td>\n",
       "      <td>0.008195</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>-0.353491</td>\n",
       "      <td>-0.192613</td>\n",
       "      <td>-0.011813</td>\n",
       "      <td>-0.093923</td>\n",
       "      <td>-0.001027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645664</th>\n",
       "      <td>3.475626</td>\n",
       "      <td>-1.446949</td>\n",
       "      <td>-1.398737</td>\n",
       "      <td>-0.003074</td>\n",
       "      <td>-0.173237</td>\n",
       "      <td>1.566787</td>\n",
       "      <td>-0.289812</td>\n",
       "      <td>-0.684234</td>\n",
       "      <td>-0.367597</td>\n",
       "      <td>-1.465663</td>\n",
       "      <td>-0.155180</td>\n",
       "      <td>-1.214700</td>\n",
       "      <td>-0.137583</td>\n",
       "      <td>2.014173</td>\n",
       "      <td>0.004905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645665</th>\n",
       "      <td>-1.247781</td>\n",
       "      <td>-1.478646</td>\n",
       "      <td>1.264406</td>\n",
       "      <td>0.045499</td>\n",
       "      <td>0.215390</td>\n",
       "      <td>-0.159233</td>\n",
       "      <td>-0.059426</td>\n",
       "      <td>-0.042416</td>\n",
       "      <td>-0.043150</td>\n",
       "      <td>-0.242519</td>\n",
       "      <td>-0.386797</td>\n",
       "      <td>-0.291047</td>\n",
       "      <td>-0.020665</td>\n",
       "      <td>-0.172150</td>\n",
       "      <td>0.014284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645666</th>\n",
       "      <td>-0.841816</td>\n",
       "      <td>2.321149</td>\n",
       "      <td>-0.540101</td>\n",
       "      <td>-0.057954</td>\n",
       "      <td>-0.551259</td>\n",
       "      <td>0.107866</td>\n",
       "      <td>-0.061544</td>\n",
       "      <td>-0.081089</td>\n",
       "      <td>-0.057524</td>\n",
       "      <td>-0.259889</td>\n",
       "      <td>-0.643144</td>\n",
       "      <td>-0.258358</td>\n",
       "      <td>-0.021054</td>\n",
       "      <td>-0.265889</td>\n",
       "      <td>-0.000574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645667</th>\n",
       "      <td>-1.212118</td>\n",
       "      <td>-1.348980</td>\n",
       "      <td>1.454567</td>\n",
       "      <td>0.096419</td>\n",
       "      <td>0.600753</td>\n",
       "      <td>0.317209</td>\n",
       "      <td>-0.123615</td>\n",
       "      <td>-0.013282</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>-0.074642</td>\n",
       "      <td>-0.526200</td>\n",
       "      <td>0.211604</td>\n",
       "      <td>0.017073</td>\n",
       "      <td>0.029616</td>\n",
       "      <td>0.014714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>645668 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              PC1        PC2        PC3         PC4        PC5        PC6  \\\n",
       "0       -0.785003   1.657681  -0.506929   -0.050599  -0.518908   0.079913   \n",
       "1       -0.802038   2.211745  -0.546086   -0.053113  -0.505308   0.063077   \n",
       "2       -0.726011   1.198838  -0.491408   -0.043707  -0.470147   0.034556   \n",
       "3       -0.772734   1.457680  -0.495099   -0.049000  -0.515639   0.077905   \n",
       "4       23.547853  12.327210  23.732811  279.533905 -44.643639 -22.102262   \n",
       "...           ...        ...        ...         ...        ...        ...   \n",
       "645663  -0.760433   1.548975  -0.507242   -0.047967  -0.493170   0.055173   \n",
       "645664   3.475626  -1.446949  -1.398737   -0.003074  -0.173237   1.566787   \n",
       "645665  -1.247781  -1.478646   1.264406    0.045499   0.215390  -0.159233   \n",
       "645666  -0.841816   2.321149  -0.540101   -0.057954  -0.551259   0.107866   \n",
       "645667  -1.212118  -1.348980   1.454567    0.096419   0.600753   0.317209   \n",
       "\n",
       "             PC7       PC8       PC9       PC10      PC11       PC12  \\\n",
       "0      -0.053267 -0.029689 -0.000764  -0.039560 -0.289287  -0.179887   \n",
       "1      -0.056795 -0.051315 -0.048846  -0.215653 -0.828292  -0.296404   \n",
       "2      -0.045349  0.019654  0.038050   0.117939 -0.202292  -0.158644   \n",
       "3      -0.051312 -0.017597  0.016456   0.025595 -0.143721  -0.148087   \n",
       "4      -0.986734  0.446772 -3.180127 -13.028477 -4.967493  14.304883   \n",
       "...          ...       ...       ...        ...       ...        ...   \n",
       "645663 -0.050211 -0.010580  0.008195   0.000508 -0.353491  -0.192613   \n",
       "645664 -0.289812 -0.684234 -0.367597  -1.465663 -0.155180  -1.214700   \n",
       "645665 -0.059426 -0.042416 -0.043150  -0.242519 -0.386797  -0.291047   \n",
       "645666 -0.061544 -0.081089 -0.057524  -0.259889 -0.643144  -0.258358   \n",
       "645667 -0.123615 -0.013282  0.024876  -0.074642 -0.526200   0.211604   \n",
       "\n",
       "            PC13      PC14       PC15  \n",
       "0      -0.011793 -0.104393  -0.001320  \n",
       "1      -0.022592 -0.271130  -0.000658  \n",
       "2      -0.007374 -0.013347  -0.001263  \n",
       "3      -0.008512 -0.050619  -0.001514  \n",
       "4       0.846526  7.171854  83.742699  \n",
       "...          ...       ...        ...  \n",
       "645663 -0.011813 -0.093923  -0.001027  \n",
       "645664 -0.137583  2.014173   0.004905  \n",
       "645665 -0.020665 -0.172150   0.014284  \n",
       "645666 -0.021054 -0.265889  -0.000574  \n",
       "645667  0.017073  0.029616   0.014714  \n",
       "\n",
       "[645668 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_components = 15\n",
    "name_cols = [f'PC{i}' for i in range(1, n_components + 1)]\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(df_train)\n",
    "reduced_train = pca.transform(df_train)\n",
    "reduced_test = pca.transform(df_test)\n",
    "reduced_train = pd.DataFrame(reduced_train, columns=name_cols)\n",
    "reduced_test = pd.DataFrame(reduced_test, columns=name_cols)\n",
    "display(reduced_train)\n",
    "display(reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = reduced_train, target_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = reduced_test, target_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val, input_dim, output_dim):\n",
    "    gate_hidden_units_options = {\n",
    "        \"16\": [16], \n",
    "        \"32\": [32], \n",
    "        \"64\": [64], \n",
    "        \"32_16\": [32, 16]\n",
    "    }\n",
    "    \n",
    "    chosen_gate_hidden_units_str = trial.suggest_categorical('gate_hidden_units', list(gate_hidden_units_options.keys()))\n",
    "    chosen_gate_hidden_units = gate_hidden_units_options[chosen_gate_hidden_units_str]\n",
    "\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = MixtureOfExperts(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_experts=output_dim,  # Number of experts equals the number of classes\n",
    "        expert_hidden_units=[32, 64, 32],\n",
    "        gate_hidden_units=chosen_gate_hidden_units,\n",
    "        num_active_experts=3,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    \n",
    "    # Initialize the expert usage logger\n",
    "    expert_usage_logger = ExpertUsageLogger(model)\n",
    "\n",
    "    # Initialize the PyTorch Lightning trainer\n",
    "    logger = TensorBoardLogger(\"logs\", name=\"MoE_experimental\")\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_f2', mode='max') \n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=300,\n",
    "        logger=logger,\n",
    "        callbacks=[lr_monitor, checkpoint_callback, expert_usage_logger],\n",
    "        accelerator='gpu',\n",
    "    )\n",
    "    \n",
    "    # Create PyTorch DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train.values, device='cuda'), torch.tensor(y_train, device='cuda')), batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.tensor(X_val.values, device='cuda'), torch.tensor(y_val, device='cuda')), batch_size=64)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_f2 = trainer.callback_metrics[\"val_f2\"].item()\n",
    "\n",
    "    return val_f2\n",
    "\n",
    "# Run the Optuna optimization\n",
    "def tune_model(X_train, y_train, X_val, y_val, input_dim, output_dim, n_trials=20):\n",
    "    gate_hidden_units_options = {\n",
    "        \"16\": [16], \n",
    "        \"32\": [32], \n",
    "        \"64\": [64], \n",
    "        \"32_16\": [32, 16]\n",
    "    }\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    \n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val, input_dim, output_dim), \n",
    "                   n_trials=n_trials)\n",
    "    \n",
    "    print(f\"Best Hyperparameters: {study.best_params}\")\n",
    "    \n",
    "    # Optionally, retrain the model with the best hyperparameters and return it\n",
    "    best_params = study.best_params\n",
    "    best_gate_hidden_units = gate_hidden_units_options[best_params['gate_hidden_units']]\n",
    "    \n",
    "    best_model = MixtureOfExperts(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_experts=output_dim, \n",
    "        expert_hidden_units=[32, 64, 32],\n",
    "        gate_hidden_units=best_gate_hidden_units,\n",
    "        num_active_experts=3,\n",
    "        dropout_rate=best_params['dropout_rate']\n",
    "    )\n",
    "    \n",
    "    # Initialize the expert usage logger\n",
    "    expert_usage_logger = ExpertUsageLogger(best_model)\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        callbacks=[expert_usage_logger],\n",
    "        accelerator='gpu'\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
    "    \n",
    "    trainer.fit(best_model, train_loader, val_loader)\n",
    "    \n",
    "    expert_usage_logger.plot_expert_usage()\n",
    "    \n",
    "    return best_model, study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-03 10:20:59,227] A new study created in memory with name: no-name-cbe5f9f2-a286-4afa-a642-e891a9e3dda6\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2024-09-03 10:20:59.748149: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-03 10:20:59.824147: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-03 10:20:59.858004: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-03 10:20:59.867777: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-03 10:20:59.919910: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-03 10:21:00.736607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | experts   | ModuleList       | 4.7 K  | train\n",
      "1 | gate      | GateModel        | 1.1 K  | train\n",
      "2 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "5.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  46%|████▌     | 22670/49060 [01:39<01:55, 228.00it/s, v_num=5, val_f2=0.000143]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "[W 2024-09-03 10:32:30,998] Trial 0 failed with parameters: {'gate_hidden_units': '64', 'dropout_rate': 0.1580665397723246} because of the following error: NameError(\"name 'exit' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 269, in advance\n",
      "    call._call_callback_hooks(trainer, \"on_train_batch_end\", batch_output, batch, batch_idx)\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
      "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/callbacks/progress/tqdm_progress.py\", line 278, in on_train_batch_end\n",
      "    _update_n(self.train_progress_bar, n)\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/callbacks/progress/tqdm_progress.py\", line 459, in _update_n\n",
      "    bar.refresh()\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/tqdm/std.py\", line 1347, in refresh\n",
      "    self.display()\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/tqdm/std.py\", line 1495, in display\n",
      "    self.sp(self.__str__() if msg is None else msg)\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/tqdm/std.py\", line 459, in print_status\n",
      "    fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/tqdm/std.py\", line 453, in fp_write\n",
      "    fp_flush()\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/tqdm/utils.py\", line 196, in inner\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/ipykernel/iostream.py\", line 609, in flush\n",
      "    if not evt.wait(self.flush_timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 629, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 331, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5704/3938687107.py\", line 71, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val, input_dim, output_dim),\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5704/3938687107.py\", line 54, in objective\n",
      "    trainer.fit(model, train_loader, val_loader)\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/fabricio/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py\", line 64, in _call_and_handle_interrupt\n",
      "    exit(1)\n",
      "    ^^^^\n",
      "NameError: name 'exit' is not defined\n",
      "[W 2024-09-03 10:32:31,002] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:269\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m--> 269\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_batch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_output, batch, batch_idx)\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:218\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/callbacks/progress/tqdm_progress.py:278\u001b[0m, in \u001b[0;36mTQDMProgressBar.on_train_batch_end\u001b[0;34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_update(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar\u001b[38;5;241m.\u001b[39mtotal):\n\u001b[0;32m--> 278\u001b[0m     \u001b[43m_update_n\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metrics(trainer, pl_module))\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/callbacks/progress/tqdm_progress.py:459\u001b[0m, in \u001b[0;36m_update_n\u001b[0;34m(bar, value)\u001b[0m\n\u001b[1;32m    458\u001b[0m bar\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 459\u001b[0m \u001b[43mbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/tqdm/std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/tqdm/std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/tqdm/std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    458\u001b[0m len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 459\u001b[0m \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/tqdm/std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    452\u001b[0m fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[0;32m--> 453\u001b[0m \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/tqdm/utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 71\u001b[0m, in \u001b[0;36mtune_model\u001b[0;34m(X_train, y_train, X_val, y_val, input_dim, output_dim, n_trials)\u001b[0m\n\u001b[1;32m     63\u001b[0m gate_hidden_units_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m16\u001b[39m], \n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m32\u001b[39m], \n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m64\u001b[39m], \n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32_16\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m]\n\u001b[1;32m     68\u001b[0m }\n\u001b[1;32m     69\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m               \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Optionally, retrain the model with the best hyperparameters and return it\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[22], line 71\u001b[0m, in \u001b[0;36mtune_model.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     63\u001b[0m gate_hidden_units_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m16\u001b[39m], \n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m32\u001b[39m], \n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m64\u001b[39m], \n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32_16\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m]\n\u001b[1;32m     68\u001b[0m }\n\u001b[1;32m     69\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     72\u001b[0m                n_trials\u001b[38;5;241m=\u001b[39mn_trials)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Optionally, retrain the model with the best hyperparameters and return it\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 54\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, X_train, y_train, X_val, y_val, input_dim, output_dim)\u001b[0m\n\u001b[1;32m     51\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(X_val\u001b[38;5;241m.\u001b[39mvalues, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), torch\u001b[38;5;241m.\u001b[39mtensor(y_val, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m     57\u001b[0m val_f2 \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_f2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/mpc/env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "tune_model(X, y, X_test, y_test, X.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
