{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ExpertModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units, dropout_rate):\n",
    "        super(ExpertModel, self).__init__()\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(input_dim, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = units\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class GateModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, hidden_units, dropout_rate):\n",
    "        super(GateModel, self).__init__()\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(input_dim, units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = units\n",
    "        layers.append(nn.Linear(input_dim, num_experts))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.model(x), dim=1)\n",
    "\n",
    "\n",
    "class MixtureOfExperts(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, num_experts, expert_hidden_units, gate_hidden_units, num_active_experts, dropout_rate, learning_rate=1e-3):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.experts = nn.ModuleList([ExpertModel(input_dim, output_dim, expert_hidden_units, dropout_rate) for _ in range(num_experts)])\n",
    "        self.gate = GateModel(input_dim, num_experts, gate_hidden_units, dropout_rate)\n",
    "        self.num_active_experts = num_active_experts\n",
    "        self.expert_usage_count = torch.zeros(num_experts, dtype=torch.float32)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)\n",
    "        gate_output = self.gate(x)\n",
    "\n",
    "        expert_usage_count_adjusted = self.expert_usage_count + 1e-10\n",
    "        importance_scores = gate_output / expert_usage_count_adjusted\n",
    "\n",
    "        top_n_expert_indices = torch.argsort(importance_scores, dim=1, descending=True)[:, :self.num_active_experts]\n",
    "        selected_expert_indices = top_n_expert_indices.view(-1)\n",
    "\n",
    "        self.expert_usage_count += torch.bincount(selected_expert_indices, minlength=len(self.experts)).float()\n",
    "\n",
    "        mask = torch.sum(F.one_hot(top_n_expert_indices, num_classes=len(self.experts)), dim=1)\n",
    "        masked_gate_output = gate_output * mask\n",
    "        normalized_gate_output = masked_gate_output / (torch.sum(masked_gate_output, dim=1, keepdim=True) + 1e-7)\n",
    "\n",
    "        masked_expert_outputs = torch.stack([expert_outputs[:, i] * normalized_gate_output[:, i].unsqueeze(1)\n",
    "                                              for i in range(len(self.experts))], dim=1)\n",
    "        final_output = torch.sum(masked_expert_outputs, dim=1)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        f2_score = fbeta_score(y.cpu().numpy(), preds.cpu().numpy(), beta=2, average='macro')\n",
    "        self.log('val_f2', f2_score, prog_bar=True, sync_dist=True)\n",
    "        return {'val_f2': f2_score}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.1, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_f2',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        self.expert_usage_count = self.expert_usage_count.to(self.device)\n",
    "\n",
    "class ExpertUsageLogger(pl.Callback):\n",
    "    def __init__(self, moe_model):\n",
    "        super(ExpertUsageLogger, self).__init__()\n",
    "        self.moe_model = moe_model\n",
    "        self.expert_usage_history = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        usage_count = self.moe_model.expert_usage_count.clone().cpu().numpy()\n",
    "        self.expert_usage_history.append(usage_count)\n",
    "\n",
    "    def plot_expert_usage(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        usage_history = torch.tensor(self.expert_usage_history)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(usage_history.shape[1]):\n",
    "            plt.plot(usage_history[:, i], label=f'Expert {i}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Expert Usage Count')\n",
    "        plt.title('Expert Usage Over Epochs')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'CIC_IoMT_2024_WiFi_MQTT_train.parquet'\n",
    "test_data_path = 'CIC_IoMT_2024_WiFi_MQTT_test.parquet'\n",
    "usage_ratio=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(train_data_path)\n",
    "df_test = pd.read_parquet(test_data_path)\n",
    "\n",
    "# Combine train and test data\n",
    "df_combined = pd.concat([df_train, df_test])\n",
    "\n",
    "display(df_train.nunique())\n",
    "df_train.info()\n",
    "# Perform stratified sampling\n",
    "df_sampled, _ = train_test_split(df_combined, train_size=usage_ratio, stratify=df_combined['label'], random_state=42)\n",
    "\n",
    "# Split back into train and test based on the original indices\n",
    "df_train: pd.DataFrame = df_sampled[df_sampled.index.isin(df_train.index)]\n",
    "df_test: pd.DataFrame = df_sampled[df_sampled.index.isin(df_test.index)]\n",
    "numeric_columns = df_train.select_dtypes(include=[np.number]).columns\n",
    "df_train[numeric_columns] = df_train[numeric_columns].astype(np.float32)\n",
    "df_test[numeric_columns] = df_test[numeric_columns].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "df_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['DHCP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [col for col in df_train.columns if col not in ['label', 'Drate']]\n",
    "\n",
    "target_train = df_train['label']\n",
    "df_train = df_train.drop(columns=['label', 'Drate'])\n",
    "target_test = df_test['label']\n",
    "df_test = df_test.drop(columns=['label', 'Drate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of missing values: {df_train.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "target_train_encoded = encoder.fit_transform(target_train.values.reshape(-1, 1))\n",
    "target_test_encoded = encoder.transform(target_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_encoded.shape, target_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = np.abs(stats.zscore(df_train[numerical_columns].astype(np.float64)))\n",
    "\n",
    "outlier_mask = np.any(z_scores > 4, axis=1)\n",
    "\n",
    "# Filter out rows with outliers\n",
    "df_train = df_train[~outlier_mask]\n",
    "target_train_encoded = target_train_encoded[~outlier_mask]\n",
    "\n",
    "print(f\"{outlier_mask.sum()} out of {len(outlier_mask)} samples were filtered out as outliers.\")\n",
    "print(f\"Number of missing values: {df_train.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df_train.mean()\n",
    "std = df_train.std()\n",
    "display(mean)\n",
    "display(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = mean + 1e-5\n",
    "std = std + 1e-5\n",
    "df_train = ((df_train - mean) / std).dropna(axis=1)\n",
    "df_test = ((df_test - mean) / std).dropna(axis=1)\n",
    "\n",
    "print(f\"Number of missing values: {df_train.isna().sum().sum()}\")\n",
    "print(f\"Number of missing values: {df_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(df_train, rowvar=False)\n",
    "upper_triangle_indices = np.triu_indices_from(corr_matrix, k=1)\n",
    "correlated_pairs = [(i, j) for i, j in zip(*upper_triangle_indices) if np.abs(corr_matrix[i, j]) >= 0.8]\n",
    "cols_train, cols_test = df_train.columns, df_test.columns\n",
    "correlated_features = set(j for _, j in correlated_pairs)\n",
    "df_train = np.delete(df_train, list(correlated_features), axis=1)\n",
    "df_test = np.delete(df_test, list(correlated_features), axis=1)\n",
    "df_train = pd.DataFrame(df_train, columns=cols_train.drop(cols_train[list(correlated_features)]))\n",
    "df_test = pd.DataFrame(df_test, columns=cols_test.drop(cols_test[list(correlated_features)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 15\n",
    "name_cols = [f'PC{i}' for i in range(1, n_components + 1)]\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(df_train)\n",
    "reduced_train = pca.transform(df_train)\n",
    "reduced_test = pca.transform(df_test)\n",
    "reduced_train = pd.DataFrame(reduced_train, columns=name_cols)\n",
    "reduced_test = pd.DataFrame(reduced_test, columns=name_cols)\n",
    "display(reduced_train)\n",
    "display(reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = reduced_train, target_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = reduced_test, target_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def objective(trial, X_train, y_train, X_val, y_val, input_dim, output_dim):\n",
    "    gate_hidden_units_options = {\n",
    "        \"16\": [16], \n",
    "        \"32\": [32], \n",
    "        \"64\": [64], \n",
    "        \"32_16\": [32, 16]\n",
    "    }\n",
    "    \n",
    "    chosen_gate_hidden_units_str = trial.suggest_categorical('gate_hidden_units', list(gate_hidden_units_options.keys()))\n",
    "    chosen_gate_hidden_units = gate_hidden_units_options[chosen_gate_hidden_units_str]\n",
    "\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = MixtureOfExperts(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_experts=output_dim,  # Number of experts equals the number of classes\n",
    "        expert_hidden_units=[32, 64, 32],\n",
    "        gate_hidden_units=chosen_gate_hidden_units,\n",
    "        num_active_experts=3,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    \n",
    "    # Initialize the expert usage logger\n",
    "    expert_usage_logger = ExpertUsageLogger(model)\n",
    "\n",
    "    # Initialize the PyTorch Lightning trainer\n",
    "    logger = TensorBoardLogger(\"logs\", name=\"MoE_experimental\")\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_f2', mode='max') \n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=300,\n",
    "        logger=logger,\n",
    "        callbacks=[lr_monitor, checkpoint_callback, expert_usage_logger],\n",
    "        accelerator='gpu',\n",
    "    )\n",
    "    \n",
    "    # Create PyTorch DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train.values, device='cuda'), torch.tensor(y_train, device='cuda')), batch_size=8192, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.tensor(X_val.values, device='cuda'), torch.tensor(y_val, device='cuda')), batch_size=8192)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_f2 = trainer.callback_metrics[\"val_f2\"].item()\n",
    "\n",
    "    return val_f2\n",
    "\n",
    "# Run the Optuna optimization\n",
    "def tune_model(X_train, y_train, X_val, y_val, input_dim, output_dim, n_trials=20):\n",
    "    gate_hidden_units_options = {\n",
    "        \"16\": [16], \n",
    "        \"32\": [32], \n",
    "        \"64\": [64], \n",
    "        \"32_16\": [32, 16]\n",
    "    }\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    \n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val, input_dim, output_dim), \n",
    "                   n_trials=n_trials)\n",
    "    \n",
    "    print(f\"Best Hyperparameters: {study.best_params}\")\n",
    "    \n",
    "    # Optionally, retrain the model with the best hyperparameters and return it\n",
    "    best_params = study.best_params\n",
    "    best_gate_hidden_units = gate_hidden_units_options[best_params['gate_hidden_units']]\n",
    "    \n",
    "    best_model = MixtureOfExperts(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_experts=output_dim, \n",
    "        expert_hidden_units=[32, 64, 32],\n",
    "        gate_hidden_units=best_gate_hidden_units,\n",
    "        num_active_experts=3,\n",
    "        dropout_rate=best_params['dropout_rate']\n",
    "    )\n",
    "    \n",
    "    # Initialize the expert usage logger\n",
    "    expert_usage_logger = ExpertUsageLogger(best_model)\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        callbacks=[expert_usage_logger],\n",
    "        accelerator='gpu'\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=4096, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=4096)\n",
    "    \n",
    "    trainer.fit(best_model, train_loader, val_loader)\n",
    "    \n",
    "    expert_usage_logger.plot_expert_usage()\n",
    "    \n",
    "    return best_model, study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_model(X, y, X_test, y_test, X.shape[1], 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
